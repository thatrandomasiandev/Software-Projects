# -*- coding: utf-8 -*-
"""Artificial Intelligence Model 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FcWpp233rPmOwaUDzemfo3qAchma1Nxr

Packages & Setup
"""

import os

notebooks = []
for root, dirs, files in os.walk('/content'):
    for fn in files:
        if fn.endswith('.ipynb'):
            notebooks.append(os.path.join(root, fn))

print("Found notebooks:")
print("\n".join(notebooks))

from google.colab import drive
drive.mount('/content/drive')

import nbformat

NOTEBOOK_PATH = "/content/drive/MyDrive/Artificial Intelligence Model 1.ipynb"
CLEANED_PATH  = NOTEBOOK_PATH.replace(".ipynb", "-cleaned.ipynb")

nb = nbformat.read(NOTEBOOK_PATH, as_version=nbformat.NO_CONVERT)

def clean_widgets(md):
    if isinstance(md, dict):
        if "widgets" in md:
            md["widgets"] = {"state": {}}
        for v in md.values():
            clean_widgets(v)
    elif isinstance(md, list):
        for elem in md:
            clean_widgets(elem)

clean_widgets(nb.metadata)
for cell in nb.cells:
    clean_widgets(cell.metadata)

nbformat.write(nb, CLEANED_PATH)
print("Cleaned notebook written to:", CLEANED_PATH)

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load Falcon-7B (fp16)
tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-7b")
model = AutoModelForCausalLM.from_pretrained(
    "tiiuae/falcon-7b",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

def chat(prompt: str, max_new_tokens: int = 50):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    out = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(out[0], skip_special_tokens=True)

print("=== Math Solver Chat ===")
print("Type 'exit' or 'quit' to stop.\n")

while True:
    q = input("You: ")
    if q.strip().lower() in {"exit","quit"}:
        print("Goodbye! ðŸ‘‹")
        break

    prompt = (
        "SYSTEM: You are an expert math solver. Solve the equation and return only the numerical answer.\n"
        f"USER: {q}\n"
        "ASSISTANT:"
    )
    resp = chat(prompt)
    ans = resp[len(prompt):].strip() if resp.startswith(prompt) else resp
    print("AI:", ans, "\n")

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_name = "EleutherAI/gpt-j-6B"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    llm_int8_threshold=6.0,
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

print(f"Loaded {model_name} â€” 4-bit quant: {model.is_loaded_in_4bit}")

import torch

def chat(prompt: str,
         max_new_tokens: int = 150,
         temperature: float = 0.7,
         top_k: int = 50,
         top_p: float = 0.9):
    """
    Generates a continuation for `prompt` using the loaded GPT-J model.
    """
    # Tokenize and move to device
    inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            pad_token_id=tokenizer.eos_token_id
        )
    # Decode and return full text
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print(chat("Hello! How are you doing today?"))

import collections
from transformers import StoppingCriteria, StoppingCriteriaList

# Pure single-turn test
prompt = "You are a helpful, concise AI assistant.\nUser: What is kinematics?\nAssistant:"
print(chat(prompt, max_new_tokens=100, temperature=0.7))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# Load GPT-J 6B in 4-bit (youâ€™ve done this before)
model_name = "EleutherAI/gpt-j-6B"
bnb = BitsAndBytesConfig(load_in_4bit=True, llm_int8_threshold=6.0)

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb,
    device_map="auto",
    trust_remote_code=True
)

def chat(prompt: str, max_new_tokens=100, temperature=0.7):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    out = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(out[0], skip_special_tokens=True)

# === Switch to Falcon-7B & Run Deterministic Chat ===

# 1. Imports
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# 2. Load Falcon-7B in FP16 (no access token required)
model_name = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# 3. Greedy chat function
def chat(prompt: str, max_new_tokens: int = 100):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    out = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,            # greedy decoding
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(out[0], skip_special_tokens=True)

# 4. Minimal interactive loop
print("=== Falcon-7B Interactive Chat ===")
print("Type 'exit' or 'quit' to stop.\n")

while True:
    user_q = input("You: ")
    if user_q.strip().lower() in {"exit", "quit"}:
        print("Hasta la vista baby ðŸ‘‹")
        break

    prompt = (
        "SYSTEM: You are a helpful, concise AI assistant.\n"
        f"USER: {user_q}\n"
        "ASSISTANT:"
    )
    response = chat(prompt, max_new_tokens=150)
    # Extract only the assistant's reply
    reply = response[len(prompt):].strip() if response.startswith(prompt) else response
    print("AI:", reply, "\n")

You: What is kinematics?
